<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Dynamic Assortment · DecisionFocusedLearningBenchmarks.jl</title><meta name="title" content="Dynamic Assortment · DecisionFocusedLearningBenchmarks.jl"/><meta property="og:title" content="Dynamic Assortment · DecisionFocusedLearningBenchmarks.jl"/><meta property="twitter:title" content="Dynamic Assortment · DecisionFocusedLearningBenchmarks.jl"/><meta name="description" content="Documentation for DecisionFocusedLearningBenchmarks.jl."/><meta property="og:description" content="Documentation for DecisionFocusedLearningBenchmarks.jl."/><meta property="twitter:description" content="Documentation for DecisionFocusedLearningBenchmarks.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">DecisionFocusedLearningBenchmarks.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><span class="tocitem">Home</span><ul><li><a class="tocitem" href="../../">Getting started</a></li><li><a class="tocitem" href="../../benchmark_interfaces/">Understanding Benchmark Interfaces</a></li></ul></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../../warcraft_tutorial/">Path-finding on image maps</a></li></ul></li><li><span class="tocitem">Benchmark problems list</span><ul><li><a class="tocitem" href="../argmax/">Argmax</a></li><li><a class="tocitem" href="../dvsp/">Dynamic Vehicle Scheduling</a></li><li class="is-active"><a class="tocitem" href>Dynamic Assortment</a><ul class="internal"><li><a class="tocitem" href="#Problem-Description"><span>Problem Description</span></a></li><li><a class="tocitem" href="#Key-Components"><span>Key Components</span></a></li><li><a class="tocitem" href="#Benchmark-Policies"><span>Benchmark Policies</span></a></li><li><a class="tocitem" href="#Decision-Focused-Learning-Policy"><span>Decision-Focused Learning Policy</span></a></li><li><a class="tocitem" href="#Reference"><span>Reference</span></a></li></ul></li><li><a class="tocitem" href="../fixed_size_shortest_path/">Shortest paths</a></li><li><a class="tocitem" href="../portfolio_optimization/">Portfolio Optimization</a></li><li><a class="tocitem" href="../ranking/">Ranking</a></li><li><a class="tocitem" href="../subset_selection/">Subset Selection</a></li><li><a class="tocitem" href="../vsp/">Stochastic Vehicle Scheduling</a></li><li><a class="tocitem" href="../warcraft/">Warcraft</a></li></ul></li><li><a class="tocitem" href="../../api/">API reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Benchmark problems list</a></li><li class="is-active"><a href>Dynamic Assortment</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Dynamic Assortment</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/JuliaDecisionFocusedLearning/DecisionFocusedLearningBenchmarks.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/JuliaDecisionFocusedLearning/DecisionFocusedLearningBenchmarks.jl/blob/main/docs/src/benchmarks/dynamic_assortment.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Dynamic-Assortment"><a class="docs-heading-anchor" href="#Dynamic-Assortment">Dynamic Assortment</a><a id="Dynamic-Assortment-1"></a><a class="docs-heading-anchor-permalink" href="#Dynamic-Assortment" title="Permalink"></a></h1><p>The Dynamic Assortment problem is a sequential decision-making benchmark where an agent must repeatedly select which subset of items to offer to customers over time. The goal is to maximize total revenue while accounting for dynamic customer preferences that evolve based on purchase history.</p><h2 id="Problem-Description"><a class="docs-heading-anchor" href="#Problem-Description">Problem Description</a><a id="Problem-Description-1"></a><a class="docs-heading-anchor-permalink" href="#Problem-Description" title="Permalink"></a></h2><h3 id="Overview"><a class="docs-heading-anchor" href="#Overview">Overview</a><a id="Overview-1"></a><a class="docs-heading-anchor-permalink" href="#Overview" title="Permalink"></a></h3><p>In the dynamic assortment problem, a retailer has access to a catalog of <span>$N$</span> items and must decide which subset of exactly <span>$K$</span> items to offer to customers at each time step. Customers make purchasing decisions according to a choice model that depends on public features <span>$x$</span>:</p><ul><li><strong>Item prices</strong>: Fixed monetary cost of each item</li><li><strong>Item features</strong>: Static characteristics of each item (size <span>$d$</span>)</li><li><strong>Hype</strong>: Dynamic popularity that increases when items are purchased recently, and decays over time if not purchased</li><li><strong>Saturation</strong>: Dynamic measure that slightly increases when specific items are purchased</li></ul><p>Both hype and saturation evolve over time based on the agent&#39;s assortment decisions and customer purchases, this providing an endogenous multistage stochastic optimization problem.</p><h3 id="Mathematical-Formulation"><a class="docs-heading-anchor" href="#Mathematical-Formulation">Mathematical Formulation</a><a id="Mathematical-Formulation-1"></a><a class="docs-heading-anchor-permalink" href="#Mathematical-Formulation" title="Permalink"></a></h3><p>The dynamic assortment problem can be formulated as a finite-horizon Markov Decision Process (MDP) with the following components:</p><p><strong>State Space</strong> <span>$\mathcal{S}$</span>: At time step <span>$t$</span>, the state <span>$s_t$</span> consists of:</p><p class="math-container">\[s_t = (p, f, h_t, \sigma_t, t, \mathcal{H}_t)\]</p><p>where:</p><ul><li><span>$p \in \mathbb{R}^N$</span> are the fixed item prices</li><li><span>$f \in \mathbb{R}^{d \times N}$</span> are the static item features</li><li><span>$h_t \in \mathbb{R}^N$</span> are the current hype levels for each item</li><li><span>$\sigma_t \in \mathbb{R}^N$</span> are the current saturation levels for each item</li><li><span>$t \in \{1, 2, \ldots, T\}$</span> is the current time step</li><li><span>$\mathcal{H}_t$</span> is the purchase history (last 5 purchases)</li></ul><p><strong>Action Space</strong> <span>$\mathcal{A}$</span>: The action at time <span>$t$</span> is an assortment selection:</p><p class="math-container">\[a_t \subseteq \{1, 2, \ldots, N\} \text{ such that } |a_t| = K\]</p><p><strong>Customer Choice Model</strong>: Given assortment <span>$a_t$</span>, customers choose according to a multinomial logit model:</p><p class="math-container">\[\forall i\in a_t,\, \mathbb{P}(i | a_t, s_t) = \frac{\exp(\theta_i(s_t))}{\sum_{j\in a_t} \exp(\theta_j(s_t)) + 1}\]</p><p class="math-container">\[\mathbb{P}(\text{no purchase} | a_t, s_t) = \frac{1}{\sum_{j\in a_t} \exp(\theta_j(s_t)) + 1}\]</p><p>where <span>$\theta_i(s_t)$</span> is the utility of item <span>$i$</span> at state <span>$s_t$</span>, computed by a hidden utility function:</p><p class="math-container">\[\theta_i(s_t) = \Phi(p_i, h_t^{(i)}, \sigma_t^{(i)}, f_{\cdot,i})\]</p><p><strong>Transition Dynamics</strong> <span>$\mathcal{P}(s_{t+1} | s_t, a_t)$</span>: After selecting assortment <span>$a_t$</span> and observing customer choice <span>$i^\star \sim \mathbb{P}(\cdot | a_t, s_t)$</span>, the state evolves as:</p><ol><li><p><strong>Hype Update</strong>: For each item <span>$i$</span>, compute a hype multiplier based on recent purchase history:</p><p class="math-container">\[m^{(i)} = 1 + \sum_{k=1}^{\min(5, |\mathcal{H}_t|)} \mathbf{1}_{i = \mathcal{H}_t[-k]} \cdot \alpha_k\]</p><p>where <span>$\mathcal{H}_t[-k]$</span> is the <span>$k$</span>-th most recent purchase, and the factors are:</p><p class="math-container">\[\alpha_1 = 0.02, \quad \alpha_2 = \alpha_3 = \alpha_4 = \alpha_5 = -0.005\]</p><p>Then update: <span>$h_{t+1}^{(i)} = h_t^{(i)} \times m^{(i)}$</span></p></li><li><p><strong>Saturation Update</strong>:</p><p class="math-container">\[\sigma_{t+1}^{(i)} = \begin{cases}
\sigma_t^{(i)} \times 1.01 &amp; \text{if } i = i^\star \\
\sigma_t^{(i)} &amp; \text{otherwise}
\end{cases}\]</p></li><li><p><strong>History Update</strong>: <span>$\mathcal{H}_{t+1} = \text{append}(\mathcal{H}_t, i^\star)$</span> (keeping last 5 purchases)</p></li></ol><p><strong>Reward Function</strong> <span>$r(s_t, a_t, s_{t+1})$</span>: The immediate reward is the revenue from the customer&#39;s purchase:</p><p class="math-container">\[r(s_t, a_t, s_{t+1}) = \begin{cases}
p_{i^\star} &amp; \text{if customer purchases item } i^\star \\
0 &amp; \text{if no purchase}
\end{cases}\]</p><p><strong>Objective</strong>: Find a policy <span>$\pi: \mathcal{S} \to \mathcal{A}$</span> that maximizes the expected cumulative reward:</p><p class="math-container">\[\max_\pi \mathbb{E}\left[\sum_{t=1}^T r(s_t, \pi(s_t), s_{t+1}) \right]\]</p><p><strong>Terminal Condition</strong>: The episode terminates after <span>$T$</span> time steps, with no terminal reward.</p><h2 id="Key-Components"><a class="docs-heading-anchor" href="#Key-Components">Key Components</a><a id="Key-Components-1"></a><a class="docs-heading-anchor-permalink" href="#Key-Components" title="Permalink"></a></h2><h3 id="[DynamicAssortmentBenchmark](@ref)"><a class="docs-heading-anchor" href="#[DynamicAssortmentBenchmark](@ref)"><a href="../../api/#DecisionFocusedLearningBenchmarks.DynamicAssortment.DynamicAssortmentBenchmark"><code>DynamicAssortmentBenchmark</code></a></a><a id="[DynamicAssortmentBenchmark](@ref)-1"></a><a class="docs-heading-anchor-permalink" href="#[DynamicAssortmentBenchmark](@ref)" title="Permalink"></a></h3><p>The main benchmark configuration with the following parameters:</p><ul><li><code>N</code>: Number of items in the catalog (default: 20)</li><li><code>d</code>: Dimension of static feature vectors (default: 2) </li><li><code>K</code>: Assortment size constraint (default: 4)</li><li><code>max_steps</code>: Number of time steps per episode (default: 80)</li><li><code>customer_choice_model</code>: linear mapping from features to utilities</li><li><code>exogenous</code>: Whether dynamics are exogenous (default: false)</li></ul><h3 id="Instance-Generation"><a class="docs-heading-anchor" href="#Instance-Generation">Instance Generation</a><a id="Instance-Generation-1"></a><a class="docs-heading-anchor-permalink" href="#Instance-Generation" title="Permalink"></a></h3><p>Each problem instance includes:</p><ul><li><strong>Prices</strong>: Random values in [1, 10] for each item, plus 0 for no-purchase</li><li><strong>Features</strong>: Random static features in [1, 10] for each item</li><li><strong>Initial State</strong>: Random starting hype and saturation values in [1, 10]</li></ul><h3 id="Environment-Dynamics"><a class="docs-heading-anchor" href="#Environment-Dynamics">Environment Dynamics</a><a id="Environment-Dynamics-1"></a><a class="docs-heading-anchor-permalink" href="#Environment-Dynamics" title="Permalink"></a></h3><p>The environment tracks:</p><ul><li>Current time step</li><li>Purchase history (last 5 purchases)</li><li>Current hype and saturation for each item  </li><li>Customer utilities computed from current state</li></ul><p><strong>State Observation</strong>: Agents observe a normalized feature vector containing:</p><ul><li>Current full features (prices, hype, saturation, static features)</li><li>Change in hype/saturation from previous step</li><li>Change in hype/saturation from initial state  </li><li>Normalized current time step</li></ul><p>All features are divided by 10 for normalization.</p><h2 id="Benchmark-Policies"><a class="docs-heading-anchor" href="#Benchmark-Policies">Benchmark Policies</a><a id="Benchmark-Policies-1"></a><a class="docs-heading-anchor-permalink" href="#Benchmark-Policies" title="Permalink"></a></h2><h3 id="Expert-Policy"><a class="docs-heading-anchor" href="#Expert-Policy">Expert Policy</a><a id="Expert-Policy-1"></a><a class="docs-heading-anchor-permalink" href="#Expert-Policy" title="Permalink"></a></h3><p>The expert policy computes the optimal assortment by brute-force enumeration:</p><ol><li>Enumerate all possible K-subsets of the N items</li><li>For each subset, compute expected revenue using the choice model</li><li>Return the subset with highest expected revenue</li></ol><p>This provides an optimal baseline but is computationally expensive.</p><h3 id="Greedy-Policy"><a class="docs-heading-anchor" href="#Greedy-Policy">Greedy Policy</a><a id="Greedy-Policy-1"></a><a class="docs-heading-anchor-permalink" href="#Greedy-Policy" title="Permalink"></a></h3><p>The greedy policy selects the K items with the highest prices, ignoring dynamic effects and customer preferences. This provides a simple baseline.</p><h2 id="Decision-Focused-Learning-Policy"><a class="docs-heading-anchor" href="#Decision-Focused-Learning-Policy">Decision-Focused Learning Policy</a><a id="Decision-Focused-Learning-Policy-1"></a><a class="docs-heading-anchor-permalink" href="#Decision-Focused-Learning-Policy" title="Permalink"></a></h2><p class="math-container">\[\xrightarrow[\text{State}]{s_t}
\fbox{Neural network $\varphi_w$}
\xrightarrow[\text{Cost vector}]{\theta}
\fbox{Top K}
\xrightarrow[\text{Assortment}]{a_t}\]</p><p><strong>Components</strong>:</p><ol><li><strong>Neural Network</strong> <span>$\varphi_w$</span>: Takes the current state <span>$s_t$</span> as input and predicts item utilities <span>$\theta = (\theta_1, \ldots, \theta_N)$</span></li><li><strong>Optimization Layer</strong>: Selects the top <span>$K$</span> items with highest predicted utilities to form the assortment <span>$a_t$</span></li></ol><h2 id="Reference"><a class="docs-heading-anchor" href="#Reference">Reference</a><a id="Reference-1"></a><a class="docs-heading-anchor-permalink" href="#Reference" title="Permalink"></a></h2><p>Based on the paper: <a href="https://arxiv.org/abs/2505.19053">Structured Reinforcement Learning for Combinatorial Decision-Making</a></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../dvsp/">« Dynamic Vehicle Scheduling</a><a class="docs-footer-nextpage" href="../fixed_size_shortest_path/">Shortest paths »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.14.1 on <span class="colophon-date" title="Tuesday 2 September 2025 16:02">Tuesday 2 September 2025</span>. Using Julia version 1.11.6.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
